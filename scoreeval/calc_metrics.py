"""
    calc_metrics.py
    ----------------
    Calculates score metrics given the path to
    the ground truth directory and the transcription
    directory!
"""

# Make necessary imports
import argparse
import pandas as pd
import numpy as np
import json
from pathlib import Path
from utils import get_note_scores, compute_mpteval, compute_activation_metrics, peamt_score, \
mv2h_eval, scoreMuster, scoreSim, scoreSer
from tqdm import tqdm

def metrics_midi_model(data, model_type: str):
    rows = []
    peamt_list = []
    act_dict = {'p': [], 'r': [], 'f': []}
    note_dict = {'p': [], 'r': [], 'f': [], 'p_off': [], 'r_off': [], 'f_off': []}
    mpteval_dict = {'cloud_diameter_corr': [], 'cloud_momentum_corr': [], 'tensile_strain_corr': [], \
                    'melody_ioi_corr': [], 'acc_ioi_corr': []}

    print(f"Calclating results for {model_type}....\n")
    for j, item in tqdm(enumerate(data), total=len(data)):
        # j = 0 is model_A and j = 1 is model_B
        # Get necessary info
        gt_midi = item["midi"]
        filename = str(Path(gt_midi).stem)

        # model_type might not be for this element so we have to check
        if model_type in item:
            pred_midi = item[model_type]
        else:
            continue

        # init row
        row = {
            ("piece", "name"): filename,
        }

        # Get metrics
        try:
            note_scores = get_note_scores(pred_midi, gt_midi)
            peamt = peamt_score(pred_midi, gt_midi)
            act_scores = compute_activation_metrics(pred_midi, gt_midi)
            mpteval_scores = compute_mpteval(pred_midi, gt_midi, result_type="all")
        except:
            # skip excerpts that generate errors
            continue

        # store info
        peamt_list.append(peamt)
        row[("PEAMT", "PEAMT")] = round(peamt, 4)
        for key, value in note_scores.items():
            note_dict[key].append(value)
            row[("Note-Level Metrics", key)] = round(value, 4) 

        for idx, key in enumerate(act_dict.keys()):         
            act_dict[key].append(act_scores[idx].item())
            row[("Act-Level Metrics", key)] = round(act_scores[idx].item(), 4)

        for idx, key in enumerate(mpteval_dict.keys()):
            mpteval_dict[key].append(mpteval_scores[key])
            row[("MPT Metrics", key)] = round(mpteval_scores[key], 4)
            # mpteval_dict[key].append(mpteval_scores[0][key])
            # row[("MPT Metrics", key)] = round(mpteval_scores[0][key], 4)
        
        rows.append(row)
    
    # Find the mean
    peamt_mean = round(sum(peamt_list)/len(peamt_list), 4)
    for key in act_dict.keys():
        act_dict[key] = round(sum(act_dict[key])/len(act_dict[key]), 4)
    for key in note_dict.keys():
        note_dict[key] = round(sum(note_dict[key])/len(note_dict[key]),4)
    for key in mpteval_dict.keys():
        mpteval_dict[key] = round(sum(mpteval_dict[key])/len(mpteval_dict[key]),4)

    # Get average of the dataset
    avg_df = {}
    for metric, value in act_dict.items():
        avg_df[("Act-Level Metrics", metric)] = value

    for metric, value in note_dict.items():
        avg_df[("Note-Level Metrics", metric)] = value

    for metric, value in mpteval_dict.items():
        avg_df[("MPT Metrics", metric)] = value

    avg_df[("PEAMT", "PEAMT")] = peamt_mean
    avg_df[("piece", "name")] = "Avg"
    rows.append(avg_df)

    result_df = pd.DataFrame(rows)
    result_df.columns = pd.MultiIndex.from_tuples(result_df.columns)
    return result_df

def metrics_midi(base_path: str, output_path: str):
    """
        Calculate the metrics for different P-MIDIs
        generated by different AMT models.

        Args:
            base_path (str): Base path to interesting directory
            output_path (str): output_path for xlsx file
    """ 
    # Ideally, we should have outputs from two models: (Transkun and Kong)
    with open(f"{base_path}/examples.json", "r", encoding="utf-8") as f:
        examples_json = json.load(f)["files"]

    results = []

    # get model types
    model_types = []
    for each in examples_json:
        for k in list(each.keys()):
            if '_pmidi' in k and k not in model_types:
                model_types.append(k)
    
    # sort the model types
    model_types = sorted(model_types)
    
    for model_type in model_types:
        metrics = metrics_midi_model(examples_json, model_type)
        results.append((metrics, model_type))
    
    with pd.ExcelWriter(f"{output_path}", engine="xlsxwriter") as w:
        for (result, result_name) in results:
            result.to_excel(w, sheet_name=result_name.replace("_pmidi", ""), index=True)

def metrics_score(base_path: str, output_path: str):
    """ 
        Calculate the metrics for for different scores 
        generated by different MIDI-to-score models.

        Args:
            base_path (str): Base path to examples directory
            output_path (str): output_path for xlsx file.
    """
    # We have four models: beyer, PM2S, Nakamaura (Pop/Classical) and Musescore
    # load the examples.json file
    with open(f"{base_path}/examples.json", "r", encoding="utf-8") as f:
        examples_json = json.load(f)["files"]
    
    # each item contains (pred_mscore, pred_xml, gt_mscore, gt_xml)
    beyer_examples = []
    pm2s_examples = []
    nakamura_examples = []
    musescore_examples = []

    for each_dict in examples_json:
        # each of these vals contain (pred_mscore, pred_xml)
        beyer_val = each_dict["beyer"]
        pm2s_val = each_dict["pm2s"]
        nak_val = each_dict["nakamura"]
        muse_val = each_dict["musescore"]
        gt_mscore = each_dict["midi_score"]
        gt_xml = each_dict["xml_score"]

        beyer_val.extend((gt_mscore, gt_xml))
        nak_val.extend((gt_mscore, gt_xml))
        pm2s_val.extend((gt_mscore, gt_xml))
        muse_val.extend((gt_mscore, gt_xml))

        beyer_examples.append(beyer_val)
        pm2s_examples.append(pm2s_val)
        nakamura_examples.append(nak_val)
        musescore_examples.append(muse_val)
    
    result_beyer = metrics_score_model(beyer_examples, 'beyer')
    result_pm2s = metrics_score_model(pm2s_examples, 'pm2s')
    result_nak = metrics_score_model(nakamura_examples, 'nak')
    result_muse = metrics_score_model(musescore_examples, 'musescore')

    with pd.ExcelWriter(f"{output_path}", engine="xlsxwriter") as w:
        result_beyer.to_excel(w, sheet_name="Beyer", index=True)
        result_pm2s.to_excel(w, sheet_name="PM2S", index=True)
        result_nak.to_excel(w, sheet_name="Nakamura", index=True)
        result_muse.to_excel(w, sheet_name="Musescore", index=True)

def metrics_score_model(data: list, model_id: str) -> pd.DataFrame:
    """
        Calculate the score-based metrics for a given
        model_id (beyer, nakamura, pm2s, musescore etc.)

        Args:
            model_id (str): model name
            data (list): list containing [midi_score, musicxml] pairs
        
        Returns:
            result (pd.DataFrame)
    """
    # Create dicts to store the results
    rows = []
    metrics_dict_scoresim =  {'e_miss': [], 'e_extra': [], 'e_dur': [],\
            'e_staff': [], 'e_stem': [], 'e_spell': []}

    metrics_dict_muster = {'e_p': [], 'e_miss': [], 'e_extra': [], 'e_onset': [], 'e_offset': []}


    metrics_dict_mv2h = {'Multi-pitch': [], 'Voice': [], 'Meter': [], 'Value': [],
                    'Harmony': [], 'MV2H': []}

    metrics_ser = []

    print(f"Calclating results for {model_id}....\n")
    for i, each in tqdm(enumerate(data), total=len(data)):
        pred_score = each[0]
        pred_xml = each[1]
        gt_score = each[2]
        gt_xml = each[3]
        filename = str(Path(pred_xml).stem) # Will be used for table display

        score_sim = scoreSim(gt_xml, pred_xml)

        # Muster has some memory leak issues; hence, we do some
        # error handling
        score_muster = scoreMuster(gt_xml, pred_xml)
        if score_muster is None:
            print(f"{filename} causes segmentation fault for MUSTER!")
            continue
        
        score_ser = scoreSer(gt_xml, pred_xml)
        score_mv2h = mv2h_eval(gt_score, pred_score)

        metrics_ser.append(score_ser['sym-er'])

        # init row
        row = {
            ("piece", "name"): filename,
        }
        row[("SER", "SER")] = round(score_ser['sym-er'], 4)

        for key in score_mv2h.keys():
            metrics_dict_mv2h[key].append(score_mv2h[key])
            row[("MV2H", key)] = round(score_mv2h[key], 4)

        for key in score_sim.keys():
            metrics_dict_scoresim[key].append(score_sim[key])
            row[("ScoreSimilarity", key)] = round(score_sim[key], 4)
        
        for key in score_muster.keys():
            metrics_dict_muster[key].append(score_muster[key])
            row[("MUSTER", key)] = round(score_muster[key], 4)
        
        rows.append(row)
        
    # Find the mean
    for key in metrics_dict_scoresim.keys():
        avg = np.mean(metrics_dict_scoresim[key])
        metrics_dict_scoresim[key] = round(avg.item(), 4)

    for key in metrics_dict_muster.keys():
        avg = np.mean(metrics_dict_muster[key])
        metrics_dict_muster[key] = round(avg.item(), 4)

    for key in metrics_dict_mv2h.keys():
        avg = np.mean(metrics_dict_mv2h[key])
        metrics_dict_mv2h[key] = round(avg.item(), 4)

    metrics_ser = round(np.mean(metrics_ser).item(), 4)

    # Get average of the dataset
    avg_df = {}
    for metric, value in metrics_dict_scoresim.items():
        avg_df[("ScoreSimilarity", metric)] = value

    for metric, value in metrics_dict_muster.items():
        avg_df[("MUSTER", metric)] = value

    for metric, value in metrics_dict_mv2h.items():
        avg_df[("MV2H", metric)] = value

    avg_df[("SER", "SER")] = metrics_ser
    avg_df[("piece", "name")] = "Avg"
    rows.append(avg_df)

    result_df = pd.DataFrame(rows)
    result_df.columns = pd.MultiIndex.from_tuples(result_df.columns)
    return result_df


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Calculate Metrics for a given model of your choice!")

    # Define arguments
    parser.add_argument('-p', '--path', required=True, help="Base path to dir. of interesting segments or examples")
    parser.add_argument('-o', '--out', required=True, help="Path to store file (e.g output.xlsx)")
    parser.add_argument('-m', '--midi', required=True, help="If 1, get midi metrics, if 0, you get score metrics,")

    args = parser.parse_args()

    if int(args.midi):
        metrics_midi(args.path, args.out)
    else:
        metrics_score(args.path, args.out)

