"""
    calc_metrics.py
    ----------------
    Calculates score metrics given the path to
    the ground truth directory and the transcription
    directory!
"""

# Make necessary imports
import argparse
import pandas as pd
import numpy as np
import json
from pathlib import Path
from music21 import converter
from metric_utils import mv2h_eval, scoreMuster, scoreSim, scoreSer
from typing import Optional

def metrics(model_id: str="trans", base_path: str="./data/interesting", config_name: str="interest.json",\
            store_dir: Optional[str]=None):
    """
        Calculate the metrics for a given Audio-to-MIDI 
        model output and the final score generated by the
        MIDI-to-score process

        Args:
        -----
            model_id (str): Supported include: trans (transkun), kong (kong), oaf (Onsets and Frames),
                            hft (hFT-Transformer)
            base_path (str): Base path to the interesting files
            config_name (str): JSON file containing informatin about the interesting files
                               and the ground truth data
            store_dir (str): Directory to store the generated xlsx file
        
        Returns
        -------- 
            Saves metrics_{model_id} to the current working directory or store_dir
    """
    # Create dicts to store the results
    rows = []
    metrics_dict_scoresim =  {'e_miss': [], 'e_extra': [], 'e_dur': [],\
            'e_staff': [], 'e_stem': [], 'e_spell': []}

    metrics_dict_muster = {'e_p': [], 'e_miss': [], 'e_extra': [], 'e_onset': [], 'e_offset': []}


    metrics_dict_mv2h = {'Multi-pitch': [], 'Voice': [], 'Meter': [], 'Value': [],
                    'Harmony': [], 'MV2H': []}

    metrics_ser = []

    BASE_PATH = base_path
    CONFIG = f"{BASE_PATH}/{config_name}"

    with open(CONFIG, "r", encoding="utf-8") as f:
        data = json.load(f)["files"]

    for i, each in enumerate(data):
        # Calculate metrics on Transkun for now
        pred_xml = each[f"{model_id}_xml"]
        gt_xml = each["xml_score"]
        pred_score = each[f"{model_id}_mscore"]
        gt_score = each["midi_score"]
        filename = str(Path(pred_xml).stem) # Will be used for table display

        score_sim = scoreSim(gt_xml, pred_xml)

        # Muster has some memory leak issues; hence, we do some
        # error handling
        score_muster = scoreMuster(gt_xml, pred_xml)
        if score_muster is None:
            print(f"{Path(each['audio']).stem} causes segmentation fault for MUSTER!")
            continue
        
        score_ser = scoreSer(gt_xml, pred_xml)
        score_mv2h = mv2h_eval(gt_score, pred_score)

        metrics_ser.append(score_ser['sym-er'])

        # init row
        row = {
            ("piece", "name"): filename,
        }
        row[("SER", "SER")] = round(score_ser['sym-er'], 4)

        for key in score_mv2h.keys():
            metrics_dict_mv2h[key].append(score_mv2h[key])
            row[("MV2H", key)] = round(score_mv2h[key], 4)

        for key in score_sim.keys():
            metrics_dict_scoresim[key].append(score_sim[key])
            row[("ScoreSimilarity", key)] = round(score_sim[key], 4)
        
        for key in score_muster.keys():
            metrics_dict_muster[key].append(score_muster[key])
            row[("MUSTER", key)] = round(score_muster[key], 4)
        
        rows.append(row)
        
    # Find the mean
    for key in metrics_dict_scoresim.keys():
        avg = np.mean(metrics_dict_scoresim[key])
        metrics_dict_scoresim[key] = round(avg.item(), 4)

    for key in metrics_dict_muster.keys():
        avg = np.mean(metrics_dict_muster[key])
        metrics_dict_muster[key] = round(avg.item(), 4)

    for key in metrics_dict_mv2h.keys():
        avg = np.mean(metrics_dict_mv2h[key])
        metrics_dict_mv2h[key] = round(avg.item(), 4)

    metrics_ser = round(np.mean(metrics_ser).item(), 4)

    # Get average of the dataset
    avg_df = {}
    for metric, value in metrics_dict_scoresim.items():
        avg_df[("ScoreSimilarity", metric)] = value

    for metric, value in metrics_dict_muster.items():
        avg_df[("MUSTER", metric)] = value

    for metric, value in metrics_dict_mv2h.items():
        avg_df[("MV2H", metric)] = value

    avg_df[("SER", "SER")] = metrics_ser
    avg_df[("piece", "name")] = "Avg"
    rows.append(avg_df)

    result_df = pd.DataFrame(rows)
    result_df.columns = pd.MultiIndex.from_tuples(result_df.columns)
    
    if store_dir is None:
        result_df.to_excel(f"metrics_{model_id}.xlsx", merge_cells=True)
    else:
        store_path = str(Path(store_dir) / f"metrics_{model_id}.xlsx")
        result_df.to_excel(store_path, merge_cells=True)

    print(result_df)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Calculate Metrics for a given model of your choice!")

    # Define arguments
    parser.add_argument('-m', '--model_id', help="Model name (e.g trans, kong, oaf, hft)")
    parser.add_argument('-p', '--path', required=False, help="Base path to dir. of interesting segments")
    parser.add_argument('-c', '--config', required=False, help="Name of config file (e.g interest.json)")
    parser.add_argument('-s', '--store_dir', required=False, help="Directory to store xlsx file.")

    args = parser.parse_args()
    if args.path is None:
        args.path = "./data/interesting"
    
    if args.config is None:
        args.config = "interest.json"

    metrics(model_id=args.model_id, base_path=args.path,\
             config_name=args.config, store_dir=args.store_dir)
